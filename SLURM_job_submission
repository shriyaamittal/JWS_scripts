####
## Example script,
####

#!/bin/bash
#SBATCH -c 1					# 1 core
#SBATCH -t 0-00:05				# Runtime of 5 minutes, in D-HH:MM format
#SBATCH -p short				# Run in short partition
#SBATCH --mem=100M				# Memory total in MB (for all cores)
#SBATCH -o jobname_%j.out			# File to which STDOUT + STDERR will be written, including job ID in filename
#SBATCH --mail-type=FAIL			# ALL email notification type
#SBATCH --mail-user=abc123@hms.harvard.edu	# Email to which notifications will be sent

# Add your job command here

# Submit as, sbatch SLURM
# or, sbatch --dependency=afterok:${job_id} ${job_filename}

Commands on HMS O2 cluster: https://wiki.rc.hms.harvard.edu/display/O2/Using+Slurm+Basic#UsingSlurmBasic-BasicSLURMcommands

squeue -u <user> === qstat

squeue -u shm976 OR squeue -u $USER
scancel <Job ID>

O2sacct, to check memory and CPU usage efficiency

####
## Example script with multiple cores in NAMD,
####

#!/bin/bash
#SBATCH -c 20					# 20 core
#SBATCH -t 0-12:00				# in D-HH:MM format
#SBATCH -p short				# Run in short partition
#SBATCH -o RS1_01_step5.1_%j.out		# File to which STDOUT + STDERR will be written, including job ID in filename

# Add your job command here

path=""
cd ${path}

/home/shm976/Sofware/NAMD_2.14b2_Linux-x86_64-multicore/namd2 +p 20 step5.1_production.inp > step5.1_production.log

# Submit as, sbatch SLURM

####
## GPU
####

#!/bin/bash
#SBATCH -c 4					# 4 core
#SBATCH -t 8:00:00				# Runtime of 8 hours
#SBATCH -p gpu_quad				# gpu_quad partition
#SBATCH --gres=gpu:1
#SBATCH -o <job>_%j.out		# File to which STDOUT + STDERR will be written, including job ID in filename

module load gcc/6.2.0 cuda/10.1
# Add your job command here
/home/shm976/Sofware/NAMD_2.14_Linux-x86_64-multicore-CUDA/namd2

####
## Check full jobname, or other info
####

sacct --helpformat
sacct --format="JobID,JobName%100" # This will show jobname upto 100 characters

####
## Submitting MD simulation jobs on O2 cluster using SLURM
##

structure="strand_12_nt_EQ1_major_unp"

for replicate in 0{1..5};
do
        prev_production_step="5.8"
        production_step="5.9"
#       production_step="4"

        job_filename=${structure}_${replicate}_${production_step}.gpuSLURM
        prev_job_filename=${structure}_${replicate}_${prev_production_step}.gpuSLURM
        touch ${job_filename}

        echo "#!/bin/bash" > ${job_filename}
        echo "#SBATCH -c 20" >> ${job_filename}
        echo "#SBATCH -t 16:00:00" >> ${job_filename}
        echo "#SBATCH -p gpu_quad" >> ${job_filename}
        echo "#SBATCH --gres=gpu:1" >> ${job_filename}
        echo "#SBATCH -o ${structure}_${replicate}_step${production_step}_%j.out" >> ${job_filename}

        echo "module load gcc/6.2.0 cuda/10.1" >> ${job_filename}

#       echo "path=\"./production_runs/rep_${replicate}/\"" >> ${job_filename}
        echo "path=\"./rep_${replicate}/\"" >> ${job_filename}
        echo "cd \${path} " >> ${job_filename}

        echo "" >> ${job_filename}
        echo "/home/shm976/Sofware/NAMD_2.14_Linux-x86_64-multicore-CUDA/namd2 +p 16 step${production_step}_production.inp > step${production_step}_production.log" >> ${job_filename}
#       echo "/home/shm976/Sofware/NAMD_2.14_Linux-x86_64-multicore-CUDA/namd2 +p 16 step4_equilibration.inp > step4_equilibration.log" >> ${job_filename}

#       cat ${job_filename}
#       sbatch ${job_filename}

        squeue -u shm976 -n ${prev_job_filename} > temp_depend_01
        tail -n 1 temp_depend_01 > temp_depend_02
        job_id="$(cut -d " " -f 11 temp_depend_02)"

#       echo ${prev_job_filename}
#       echo ${job_id}

#       cat ${job_filename}
        sbatch --dependency=afterany:${job_id} ${job_filename}
done

squeue -u $USER
